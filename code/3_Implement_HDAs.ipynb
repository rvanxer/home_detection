{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb564b56-3918-4a8f-b82d-538e1058be30",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8fcd7a71-6426-40e9-a610-91fc942e1f19",
   "metadata": {
    "tags": [],
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/umni2/a/umnilab/users/verma99/mk/home_detection/code\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b623cad5",
   "metadata": {},
   "source": [
    "**New `meanshift`**: To speed up the performance of the clustering algorithm, some key redundancies in the existing implementation of `sklearn.cluster.MeanShift` were removed. These include:\n",
    "* Removing the labeling part which involves assigning cluster labels to all the sample points. This is not necessary for home detection since we only care about the cluster center. This prevents computation of distances between the cluster centers and the sample points using the BallTree algorithm, which is a massive speedup.\n",
    "* Removing the cluster merging part. Cluster merging involves absorbing smaller clusters in close vicinity (within radius threshold) to larger clusters. Since we only care about the largest cluster and the largest cluster is always included in this step, there is no need to perform this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4cd6c50c-3583-49c1-9939-d41273231940",
   "metadata": {
    "tags": [],
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "import clustering\n",
    "from mobilkit.umni import *\n",
    "from setup import P, Region, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "876d7395-4783-454b-972a-ebf7dd236276",
   "metadata": {
    "tags": [],
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "import contextily as ctx\n",
    "import haversine as hs\n",
    "import shapely\n",
    "from sklearn.cluster import AgglomerativeClustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "97b0b54c-f8af-46b3-b076-c97120695f24",
   "metadata": {
    "tags": [],
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/06/27 16:36:53 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "SP.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77914129-6d3a-459e-8712-05f297afd141",
   "metadata": {},
   "source": [
    "## Load datasets and regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "74c5a5d2-3e79-4acb-b349-e8a77a86df1b",
   "metadata": {
    "tags": [],
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset D2(Baton Rouge: 2021-08-26 - 2021-09-07)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets = [Dataset(k, *v.values()) for k, v in P.params.get('datasets').items()]\n",
    "ds2 = datasets[1]; ds2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e4c27684-c255-4a76-8423-dcd89e1bc17c",
   "metadata": {
    "tags": [],
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Region(Austin), Region(Baton Rouge), Region(Houston), Region(Indianapolis)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regions = sorted(list({x.region for x in datasets}), key=lambda x: x.name)\n",
    "regions"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e2fb7eca-9510-4e1c-949a-bb77e57be66a",
   "metadata": {
    "tags": []
   },
   "source": [
    "import shutil\n",
    "mapper = dict(A0='A1', A1='A3', A2='A4', A3='A5', A4='A2')\n",
    "for ds in datasets:\n",
    "    for f in (ds.data / 'sensitivity').glob('**/A*'):\n",
    "        new = f\n",
    "        # new = f.parent / mapper[f'A{f.stem[-1]}']\n",
    "        # new = f.parent / f'temp{f.stem[-1]}'\n",
    "        # new_base = mapper['A'+f.stem[-1]]\n",
    "        # new = f.parent / f'{new_base}.parquet'\n",
    "        print(f, new, sep='\\n-> ')\n",
    "        # shutil.move(str(f), str(new))\n",
    "    print('_' * 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc6029e-400f-4f67-8d48-b4b8a5c31488",
   "metadata": {},
   "source": [
    "# Get home locations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b415ebf6-712e-46da-a2ba-a304d65403e7",
   "metadata": {},
   "source": [
    "## $A_1$: Centroid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0231c79d-825d-4874-9bb4-8d4db3505713",
   "metadata": {
    "tags": [],
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "def get_homes_centroid(ds, save=True, overwrite=False):\n",
    "    outpath = ds.data / 'homes/A1.parquet'\n",
    "    if outpath.exists() and not overwrite:\n",
    "        return SP.read_parquet(outpath)\n",
    "    print(f'Processing {ds}')\n",
    "    start_time = dt.datetime.now()\n",
    "    df = SP.read_parquet(ds.data / 'night_pings')\n",
    "    df = df.select(UID, *[\n",
    "        F.udf(lambda x: sum(x) / len(x), T.float)(x).alias(x) \n",
    "        for x in [LON, LAT]]).sort(UID)\n",
    "    if save:\n",
    "        df = df.toPandas()\n",
    "        df.to_parquet(U.mkfile(outpath))\n",
    "    print(f'Runtime: {dt.datetime.now() - start_time}')\n",
    "    return df\n",
    "\n",
    "# %time x = get_homes_centroid(ds2); x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5938601d-4a7f-4603-aa82-8735c19cf11a",
   "metadata": {
    "tags": [],
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# total time for aus + baro + hous + indy + sant = 41s\n",
    "# t=1:32 {1=>0:16, 2=>0:04, 3=>0:32, 4=>0:07, 5=>0:06, 6=>0:10, 7=>0:05, 8=>0:12}\n",
    "for ds in datasets:\n",
    "    get_homes_centroid(ds, overwrite=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce16ecb0-0d56-4d17-9ead-af5b188501c3",
   "metadata": {},
   "source": [
    "## $A_2$: Grid frequency-based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a8f6dc9f",
   "metadata": {
    "tags": [],
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "grid_cell_size = 20 # meters\n",
    "P.params.set({'algorithms': {'grid_frequency': {'cell_size': grid_cell_size}}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "32a47c4b-b9fc-40df-9d60-f4315f4c46ad",
   "metadata": {
    "tags": [],
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "def get_homes_grid_freq(ds, cell_size=grid_cell_size, save=True, overwrite=False):\n",
    "    outfile = ds.data / 'homes/A2.parquet'\n",
    "    if outfile.exists() and not overwrite:\n",
    "        return SP.read_parquet(outfile)\n",
    "    print(f'Processing {ds}')\n",
    "    start_time = dt.datetime.now()\n",
    "    bbox = shapely.box(*ds.region.bbox)\n",
    "    minLon, minLat, maxLon, maxLat = ds.region.bbox\n",
    "    bbox = Gdf({'geometry': [bbox]}, crs=CRS_DEG)\n",
    "    minX, minY, maxX, maxY = bbox.to_crs(CRS_M).bounds.iloc[0]\n",
    "    scaleX = (maxX - minX) / (maxLon - minLon) / cell_size\n",
    "    scaleY = (maxY - minY) / (maxLat - minLat) / cell_size\n",
    "    nCols = int(np.ceil((maxX - minX) / cell_size))\n",
    "    nRows = int(np.ceil((maxY - minY) / cell_size))\n",
    "    df = SP.read_parquet(ds.data / 'night_pings')\n",
    "    df = df.groupby(UID).agg(*[F.flatten(F.collect_list(x)).alias(x) for x in [LON, LAT]])\n",
    "    \n",
    "    def udf(x, y):\n",
    "        cx = np.floor((Arr(x) - minLon) * scaleX)\n",
    "        cy = np.floor((Arr(y) - minLat) * scaleY)\n",
    "        cells, freq = np.unique(np.vstack([cx, cy]).T, axis=0, return_counts=True)\n",
    "        cx, cy = cells[freq.argmax()]\n",
    "        return int(cx), int(cy)\n",
    "    df = df.select(UID, F.udf(udf, T.array(T.int64))(LON, LAT).alias('cell_id'))\n",
    "    df = df.select(UID, *[F.col('cell_id')[i].alias(x) for i, x in enumerate(['cx', 'cy'])])\n",
    "    df = df.withColumn(LON, minLon + (F.col('cx') + 0.5) * ((maxLon - minLon) / nCols))\n",
    "    df = df.withColumn(LAT, minLat + (F.col('cy') + 0.5) * ((maxLat - minLat) / nRows))\n",
    "    df = df.select(UID, *[F.col(x).cast(T.float).alias(x) for x in [LON, LAT]])\n",
    "    if save:\n",
    "        df = df.toPandas()\n",
    "        df.to_parquet(U.mkfile(outfile))\n",
    "    print(f'Runtime: {dt.datetime.now() - start_time}')\n",
    "    return df\n",
    "\n",
    "# %time x = get_homes_grid_freq(ds2, overwrite=True, save=False); x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "db33ebbf-15b8-4ff9-93ea-f28e5e358aff",
   "metadata": {
    "tags": [],
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# t=1:51 ({1=>0:11, 2=>0:04, 3=>0:42, 4=>0:08, 5=>0:07, 6=>0:14, 7=>0:07, 8=>0:17})\n",
    "for ds in datasets:\n",
    "    get_homes_grid_freq(ds, overwrite=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90d6fb68-d93b-4ee6-979a-8b02348a5669",
   "metadata": {},
   "source": [
    "## A3 & A4: Meanshift clustering\n",
    "In the case of $A_2$, for each slot, the pings are aggregated to just the mean \"super-ping\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aa34ac2e-8723-4e53-9151-e2b9265f4bec",
   "metadata": {
    "tags": [],
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "meanshift_radius = 250 # meters\n",
    "meanshift_params = dict(bin_seeding=True, min_bin_freq=2, max_iter=50)\n",
    "superping_slot_size = 30 * 60 # seconds\n",
    "P.params.set({'algorithms': {'meanshift': {'radius': meanshift_radius} | meanshift_params,\n",
    "              'superping': {'slot_size': superping_slot_size}}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "223dbb38-2e5e-4019-ab5c-c698385251f6",
   "metadata": {
    "tags": [],
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "def get_homes_meanshift(ds, use_superpings, radius=meanshift_radius, \n",
    "                        slot_size=superping_slot_size, \n",
    "                        params=meanshift_params, overwrite=False):\n",
    "    outpath = ds.data / 'homes/{}.parquet'.format(\n",
    "        'A4' if use_superpings else 'A3')\n",
    "    if outpath.exists() and not overwrite:\n",
    "        return SP.read_parquet(outpath)\n",
    "    print(f'Processing {ds}')\n",
    "    start_time = dt.datetime.now()\n",
    "    _, miny, _, maxy = ds.region.bbox\n",
    "    bandwidth = mk.geo.dist_m2deg(radius, (miny + maxy) / 2)\n",
    "    \n",
    "    def udf(x, y):\n",
    "        model = clustering.MeanShift(bandwidth=bandwidth, **params)\n",
    "        model.fit(np.vstack([x, y]).T)\n",
    "        cx, cy = [float(x) for x in model.cluster_center]\n",
    "        return (cx, cy, float(model.used_mean))\n",
    "    df = SP.read_parquet(ds.data / 'night_pings')\n",
    "    if use_superpings:\n",
    "        df = df.select(UID, F.arrays_zip(LON, LAT, TS).alias('xyt'))\n",
    "        df = df.select(UID, F.explode('xyt').alias('xyt'))\n",
    "        df = df.select(UID, *[F.col('xyt')[x].alias(x) for x in [LON, LAT, TS]])\n",
    "        df = df.withColumn('slot', F.udf(lambda t: int(t / slot_size), T.int16)(TS))\n",
    "        df = df.groupby(UID, 'slot').agg(\n",
    "            *[F.mean(x).cast(T.float).alias(x) for x in [LON, LAT]])\n",
    "        df = df.groupby(UID).agg(*[F.collect_list(x).alias(x) for x in [LON, LAT]])\n",
    "    df = df.withColumn('home', F.udf(udf, T.array(T.float))(LON, LAT))\n",
    "    df = df.select(UID, *[F.col('home')[i].alias(x) for i, x \n",
    "                          in enumerate([LON, LAT, 'used_mean'])])\n",
    "    df = df.withColumn('used_mean', F.col('used_mean').cast(T.bool))\n",
    "    df.toPandas().to_parquet(U.mkfile(outpath))\n",
    "    print(f'Runtime: {dt.datetime.now() - start_time}')\n",
    "    return df\n",
    "    \n",
    "# %time x = get_homes_meanshift(ds2, overwrite=True); x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28156024",
   "metadata": {},
   "source": [
    "### $A_3$: All-time clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4f7e24ab-2786-4f83-8121-503fa9bf1798",
   "metadata": {
    "tags": [],
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# t=13:56 (1=>1:11, 2=>0:22, 3=>3:22, 4=>1:21, 5=>1:31, 6=>2:43, 7=>0:51, 8=>2:34)\n",
    "for ds in datasets:\n",
    "    get_homes_meanshift(ds, use_superpings=False, overwrite=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0004610-d95a-4dfb-8c06-921b0ac36edc",
   "metadata": {},
   "source": [
    "### $A_4$: Superping clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a5ade17b-e254-471d-88c6-9dde1c204e0a",
   "metadata": {
    "tags": [],
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# t=14:50 ({1=>1:28, 2=>0:28, 3=>4:54, 4=>1:11, 5=>1:14, 6=>2:17, 7=>0:50, 8=>2:28})\n",
    "for ds in datasets:\n",
    "    get_homes_meanshift(ds, use_superpings=True, overwrite=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0906091-a18c-4e56-94ea-d7c45e670fe0",
   "metadata": {},
   "source": [
    "## A3: Stay point method\n",
    "From [Sagedhinasr et al. (2019)](https://doi.org/10.1061/9780784482438.002)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87fbaaac-889a-4aa7-8817-11f681aea15c",
   "metadata": {},
   "source": [
    "### Generate stay points\n",
    "From [Li et al. (2008)](https://dl.acm.org/doi/10.1145/1463434.1463477)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9aa03341",
   "metadata": {
    "tags": [],
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "base_day_hrs = (7, 19) # 7am â€“ 7pm\n",
    "min_total_nightly_pts = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3ae56159",
   "metadata": {
    "tags": [],
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "stay_pt_method_params = {\n",
    "    # distance threshold (meters) for stay point detection\n",
    "    'dist_thresh': 250,\n",
    "    # time threshold (in seconds) for stay point detection\n",
    "    'time_thresh': 30 * 60,\n",
    "    # maximum intra-cluster distance (meters) for a stay region\n",
    "    'intra_clust_dist': 250,\n",
    "    # minimum total dwell time of a stay region (seconds) to \n",
    "    # be considered an eligible home location\n",
    "    'min_total_dwell': 24 * 3600, # seconds\n",
    "    # minimum dwell time (seconds) during night to be\n",
    "    # considered eligible an eligible home location\n",
    "    'min_night_dwell': 3 * 3600, # seconds\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c8d866a9-0348-4364-8c7e-413a3d4ac141",
   "metadata": {
    "tags": [],
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "def get_stay_points(x, y, t, dist_thresh, time_thresh):\n",
    "    stay_pts = []\n",
    "    i = 0\n",
    "    while i < len(x):\n",
    "        j = i + 1\n",
    "        while j < len(x):\n",
    "            dist = float(hs.haversine((y[i], x[i]), (y[j], x[j]), unit='m'))\n",
    "            if dist > dist_thresh:\n",
    "                if t[j] - t[i] > time_thresh:\n",
    "                    stay_pts.append({\n",
    "                        LON: sum(x[i:j]) / (j - i),\n",
    "                        LAT: sum(y[i:j]) / (j - i),\n",
    "                        TS + '_arr': t[i],\n",
    "                        TS + '_dep': t[j]})\n",
    "                i = j\n",
    "                break\n",
    "            j += 1\n",
    "        if j == len(x):\n",
    "            break\n",
    "    return stay_pts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b3c6db-c9b1-4053-b873-e5e4f200f8a9",
   "metadata": {},
   "source": [
    "### Generate stay regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f940a850-f41d-4b9d-b50b-3fb9c5d708a9",
   "metadata": {
    "tags": [],
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "def get_stay_regions_homes(df, intra_clust_dist, linkage='complete'):\n",
    "    if isinstance(df, Sdf):\n",
    "        df = df.toPandas()\n",
    "    df = mk.geo.pdf2gdf(df, LON, LAT, CRS_DEG).to_crs(CRS_M)\n",
    "    pts = mk.geo.gdf2pdf(df, LON, LAT, CRS_M).set_index(df[UID])\n",
    "    model = AgglomerativeClustering(\n",
    "        distance_threshold=intra_clust_dist, n_clusters=None,\n",
    "        linkage=linkage, compute_full_tree=True)\n",
    "    homes = []\n",
    "    for uid, df in tqdm(pts.groupby(UID)):\n",
    "        if len(df) == 1:\n",
    "            homes.append({UID: uid, LON: df[LON].iloc[0], \n",
    "                          LAT: df[LAT].iloc[0], 'used_mean': True})\n",
    "        else:\n",
    "            model.fit(df.values)\n",
    "            clusts, freq = np.unique(model.labels_, return_counts=True)\n",
    "            home_clust = clusts[freq.argmax()]\n",
    "            xy = df[model.labels_ == home_clust].mean(axis=0)\n",
    "            homes.append({UID: uid, LON: xy[LON], LAT: xy[LAT], \n",
    "                          'used_mean': False})\n",
    "    homes = mk.geo.pdf2gdf(Pdf(homes), LON, LAT, CRS_M)\n",
    "    xy = mk.geo.gdf2pdf(homes.to_crs(CRS_DEG), LON, LAT)\n",
    "    homes = pd.concat([homes[UID], xy, homes['used_mean']], axis=1)\n",
    "    return homes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c15263b1-31ef-422f-bd6c-e0c3f6804778",
   "metadata": {},
   "source": [
    "### Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "52448dac-5653-4816-98c2-546522de4a1f",
   "metadata": {
    "tags": [],
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "def get_homes_stay_pts(ds, dist_thresh, time_thresh,\n",
    "                       min_total_dwell, min_night_dwell,\n",
    "                       intra_clust_dist, day_hrs,\n",
    "                       save=True, overwrite=False):\n",
    "    outpath = ds.data / 'homes/A5.parquet'\n",
    "    if outpath.exists() and not overwrite:\n",
    "        return SP.read_parquet(outpath)\n",
    "    print(f'Processing {ds}')\n",
    "    start_time = dt.datetime.now()\n",
    "    # collect pings\n",
    "    df = []\n",
    "    for date in ds.dates:\n",
    "        d = SP.read_parquet(ds.data / f'pings/{date}')\n",
    "        nDays = (date - ds.start).days\n",
    "        def add_day(t): return [t + nDays * 86400 for t in t]\n",
    "        d = d.withColumn(TS, F.udf(add_day, T.array(T.float))(TS))\n",
    "        df.append(d)\n",
    "    df = reduce(Sdf.union, df)\n",
    "    df = df.groupby(UID).agg(*[F.flatten(F.collect_list(x)).alias(x) \n",
    "                               for x in [LON, LAT, TS]])\n",
    "    # get stay points\n",
    "    df = df.select(UID, F.udf(\n",
    "        lambda x, y, t: get_stay_points(x, y, t, dist_thresh, time_thresh), \n",
    "        T.array(T.map(T.str, T.float)))(LON, LAT, TS).alias('stay_pts'))\n",
    "    df = (df.select(UID, F.explode('stay_pts').alias('stay_pt'))\n",
    "          .select(UID, *[F.col('stay_pt')[x].alias(x) for x in \n",
    "                         [LON, LAT, 'ts_arr', 'ts_dep']]))\n",
    "    # filter stay points\n",
    "    df = df.withColumn('dwell_time', F.col('ts_dep') - F.col('ts_arr'))\n",
    "    df1 = df.filter(f'dwell_time >= {min_total_dwell}')\n",
    "    morning_start, night_start = [x * 3600 for x in day_hrs]\n",
    "    df = (df.filter(f'dwell_time < {min_total_dwell}')\n",
    "          .withColumn('t1', morning_start - (F.col('ts_arr')))\n",
    "          .withColumn('t2', morning_start - (F.col('ts_dep') % 86400))\n",
    "          .withColumn('t3', (F.col('ts_arr') % 86400) - night_start)\n",
    "          .withColumn('t4', (F.col('ts_dep') % 86400) - night_start))\n",
    "    for t in ['t1','t2','t3','t4']:\n",
    "        df = df.withColumn(t, F.when(F.col(t) < 0, 0).otherwise(F.col(t)))\n",
    "    df = df.withColumn('night_dwell', F.expr('t1 + t2 + t3 + t4'))\n",
    "    df = df.filter(f'night_dwell >= {min_night_dwell}')\n",
    "    df = df.select(UID, LON, LAT).union(df1.select(UID, LON, LAT))\n",
    "    # get the stay regions (clustering) and idenitfy the home location\n",
    "    homes = get_stay_regions_homes(df, intra_clust_dist)\n",
    "    if save:\n",
    "        homes.to_parquet(outpath)\n",
    "    print(f'Runtime: {dt.datetime.now() - start_time}')\n",
    "    return homes\n",
    "\n",
    "# %time x = get_homes_stay_pts(ds2, day_hrs=base_day_hrs, save=False, **stay_pt_method_params); x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0d729d68-59d7-4352-8f5d-a14c214345ed",
   "metadata": {
    "tags": [],
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# t=54:09 ({1=>4:58, 2=>1:47, 3=>,12:54 4=>5:27, 5=>5:28, 6=>10:00, 7=>3:34, 8=>10:01})\n",
    "for ds in datasets:\n",
    "    get_homes_stay_pts(ds, overwrite=False, day_hrs=base_day_hrs, **stay_pt_method_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "692bfc8b-6d06-4ef9-9379-e89caf7bda6e",
   "metadata": {},
   "source": [
    "# Visualize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d77a594b-fe17-4e55-8afb-0de29c1328e3",
   "metadata": {},
   "source": [
    "## Home locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a7908f60-358d-4c5e-9800-9e7c4f449540",
   "metadata": {
    "tags": [],
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "algos = ['A1', 'A2', 'A3', 'A4', 'A5']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fb3cb498-bfdf-4194-950a-2a61da8d74a8",
   "metadata": {
    "tags": [],
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "def plot_home_locs(rgn, homes_var, grid_size=500, vmax=None,\n",
    "                   size=(12, 12), dpi=120, markersize=20, cmap='Reds'):\n",
    "    \"\"\"\n",
    "    grid_size: float\n",
    "        Size of the grid cell (meters)\n",
    "    \"\"\"\n",
    "    h = getattr(rgn, 'h' + homes_var).copy()\n",
    "    h = mk.geo.pdf2gdf(h, LON, LAT, CRS_DEG).to_crs(CRS_M)\n",
    "    h = pd.concat([h, mk.geo.gdf2pdf(h, 'x', 'y', CRS_M)], axis=1).dropna()\n",
    "    for x in ['x', 'y']:\n",
    "        h[x] = (h[x] / grid_size).astype(int) * grid_size\n",
    "    h = h.groupby(['x', 'y']).size().rename('n_users').reset_index()\n",
    "    h = mk.geo.pdf2gdf(h, 'x', 'y', CRS_M)\n",
    "    minx, miny, maxx, maxy = rgn.cnty.to_crs(CRS_M).unary_union.bounds\n",
    "    ax = U.plot(size=size, dpi=dpi, xeng=1, yeng=1, title=f'Method: {homes_var}',\n",
    "                # xlim=(minx, maxx), ylim=(miny, maxy),\n",
    "                xlab='Distance from westernmost edge (km)',\n",
    "                ylab='Distance from southernnmost edge (km)')\n",
    "    boundary = rgn.boundary.to_crs(CRS_M)\n",
    "    h = gpd.sjoin(h, boundary, predicate='within')\n",
    "    h.plot(ax=ax, column='n_users', cmap=cmap, markersize=markersize,\n",
    "           legend=True, alpha=0.5, vmax=vmax, zorder=100, marker='s', edgecolor='none',\n",
    "           legend_kwds=dict(shrink=0.5, label='No. of users'))\n",
    "    boundary.plot(ax=ax, edgecolor='b', facecolor='none', linewidth=2)\n",
    "    # rgn.cnty.to_crs(CRS_M).plot(ax=ax, facecolor='none', edgecolor='k',\n",
    "    #                             linewidth=2, zorder=101)\n",
    "    ctx.add_basemap(ax=ax, source=ctx.providers.Stamen.TonerLite, crs=CRS_M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d578399b-e15b-48ad-b8ee-6cd46d2c321a",
   "metadata": {
    "tags": [],
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "def plot_home_locs_diff(rgn, grid_size=500, vmin=None, vmax=None,\n",
    "                        size=(12, 12), dpi=120, markersize=20, cmap='RdBu'):\n",
    "    \"\"\"\n",
    "    grid_size: float\n",
    "        Size of the grid cell (meters)\n",
    "    \"\"\"\n",
    "    hold = getattr(rgn, 'hA1').copy()\n",
    "    hnew = getattr(rgn, 'hA2').copy()\n",
    "    hold = mk.geo.pdf2gdf(hold, LON, LAT, CRS_DEG).to_crs(CRS_M)\n",
    "    hnew = mk.geo.pdf2gdf(hnew, LON, LAT, CRS_DEG).to_crs(CRS_M) \n",
    "    hold = pd.concat([hold, mk.geo.gdf2pdf(hold, 'x', 'y', CRS_M)], axis=1).dropna()\n",
    "    hnew = pd.concat([hnew, mk.geo.gdf2pdf(hnew, 'x', 'y', CRS_M)], axis=1).dropna()\n",
    "    for x in ['x', 'y']:\n",
    "        hold[x] = (hold[x] / grid_size).astype(int) * grid_size\n",
    "        hnew[x] = (hnew[x] / grid_size).astype(int) * grid_size\n",
    "    hold = hold.groupby(['x', 'y']).size().rename('n_old').fillna(0)\n",
    "    hnew = hnew.groupby(['x', 'y']).size().rename('n_new').fillna(0)\n",
    "    h = pd.concat([hold, hnew], axis=1).fillna(0).reset_index()\n",
    "    h['delta'] = h['n_new'] - h['n_old']\n",
    "    h = mk.geo.pdf2gdf(h, 'x', 'y', CRS_M)\n",
    "    minx, miny, maxx, maxy = rgn.cnty.to_crs(CRS_M).unary_union.bounds\n",
    "    ax = U.plot(size=size, dpi=dpi, xeng=1, yeng=1,\n",
    "                # xlim=(minx, maxx), ylim=(miny, maxy),\n",
    "                xlab='Distance from westernmost edge (km)',\n",
    "                ylab='Distance from southernnmost edge (km)')\n",
    "    boundary = rgn.boundary.to_crs(CRS_M)\n",
    "    h = gpd.sjoin(h, boundary, predicate='within')\n",
    "    h.plot(ax=ax, column='delta', cmap=cmap, markersize=markersize,\n",
    "           legend=True, alpha=0.5, vmin=vmin, vmax=vmax, zorder=100, marker='s', edgecolor='none',\n",
    "           legend_kwds=dict(shrink=0.5, label='No. of users'))\n",
    "    boundary.plot(ax=ax, edgecolor='b', facecolor='none', linewidth=2)\n",
    "    ctx.add_basemap(ax=ax, source=ctx.providers.Stamen.TonerLite, crs=CRS_M)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dd42b9bc-c247-4778-902b-f64f2ccf5d5c",
   "metadata": {
    "tags": [],
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "def plot_representativeness_cbg(rgn, homes_var, vmin=0, vmax=15,\n",
    "                                size=(10, 10), dpi=120, cmap='magma_r'):\n",
    "    homes = getattr(rgn, 'h' + homes_var).copy()\n",
    "    homes = mk.geo.pdf2gdf(homes, LON, LAT, CRS_DEG)\n",
    "    acs = gpd.read_file(rgn.data / 'geometry/acs.gpkg').query('popu > 0')\n",
    "    homes = gpd.sjoin(homes, acs[['geometry', 'geoid']], predicate='within')\n",
    "    n_users = homes.groupby('geoid').size().rename('n_users').reset_index()\n",
    "    acs = acs.merge(n_users, on='geoid')[['county', 'n_users', 'popu', 'geometry']]\n",
    "    acs['repres'] = (acs['n_users'] * 100) / acs['popu']\n",
    "    ax = U.plot(size=size, dpi=dpi, axoff=1)\n",
    "    acs = gpd.sjoin(acs, rgn.boundary)\n",
    "    acs.to_crs(CRS_M).plot(\n",
    "        ax=ax, column='repres', cmap=cmap, legend=True, \n",
    "        vmin=vmin, vmax=vmax, edgecolor='none', # classification_kwds=dict(bins=np.arange(vmin, vmax+1)),\n",
    "        legend_kwds=dict(shrink=0.5, label='Data representativeness (%)'))\n",
    "    counties = acs.groupby('county')['geometry'].agg(lambda x: x.unary_union).set_crs(CRS_DEG)\n",
    "    counties.to_crs(CRS_M).plot(ax=ax, facecolor='none', edgecolor='k', linewidth=2)\n",
    "    rgn.boundary.to_crs(CRS_M).plot(ax=ax, facecolor='none', edgecolor='b', linewidth=2)\n",
    "\n",
    "# plot_representativeness_cbg(regions[-1], 'A4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "373b7773-4cb0-461c-acff-d783c5d483d5",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
